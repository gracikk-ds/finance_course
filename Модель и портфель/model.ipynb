{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 8, 5\n",
    "%pylab inline\n",
    "\n",
    "import datetime\n",
    "import dateutil.relativedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix as cm\n",
    "from sklearn.metrics import precision_score, accuracy_score\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction import DictVectorizer as DV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pd.read_excel('model_data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Строим целевые переменные \n",
    "\n",
    "model['y_3'] = model['shifted_3'] - model['shifted_3_sec']\n",
    "model['y_3'].where(model['y_3']<0, 1, inplace = True)\n",
    "model['y_3'].where(model['y_3']>0, 0, inplace = True)\n",
    "\n",
    "\n",
    "model['y_6'] = model['shifted_6'] - model['shifted_6_sec']\n",
    "model['y_6'].where(model['y_6']<0, 1, inplace = True)\n",
    "model['y_6'].where(model['y_6']>0, 0, inplace = True)\n",
    "\n",
    "\n",
    "model['y_12'] = model['shifted_12'] - model['shifted_12_sec']\n",
    "model['y_12'].where(model['y_12']<0, 1, inplace = True)\n",
    "model['y_12'].where(model['y_12']>0, 0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reset_index(inplace = True, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spisok = ['0',   '1',   '2',   '5',   '6',   '7',   '9',   '14',   '15',\n",
    "   '17',   '21',   '22',   '23',   '24',   '25',   '26',   '31',   '32',   '33',\n",
    "   '38',   '39',   '40',   '41',   '42',   '43',   '44',   '45',   '47',   '48',   '49']\n",
    "spisok = map(int, spisok)\n",
    "l = list(spisok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Для 17 года\n",
    "model_train_2017 = model.iloc[:4545, :]\n",
    "model_test_2017 = model.iloc[4545:, :]\n",
    "\n",
    "# Для 16 года\n",
    "model_train_2016 = model.iloc[:4200, :]\n",
    "model_test_2016 = model.iloc[4200:4445, :]\n",
    "\n",
    "model_train_2017 = model_train_2017.sample(frac=1, random_state = 1)\n",
    "model_train_2016 = model_train_2016.sample(frac=1, random_state = 1)\n",
    "\n",
    "Y_train_2017 = model_train_2017.iloc[:, -12:]\n",
    "Y_test_2017 = model_test_2017.iloc[:, -12:]\n",
    "Y_6_train_2017 = Y_train_2017.iloc[:, -2:-1]\n",
    "Y_6_test_2017 = Y_test_2017.iloc[:, -2:-1]\n",
    "\n",
    "Y_train_2016 = model_train_2016.iloc[:, -12:]\n",
    "Y_test_2016 = model_test_2016.iloc[:, -12:]\n",
    "Y_6_train_2016 = Y_train_2016.iloc[:, -2:-1]\n",
    "Y_6_test_2016 = Y_test_2016.iloc[:, -2:-1]\n",
    "\n",
    "X_train_2017 = model_train_2017.iloc[:, :-12]\n",
    "X_test_2017 = model_test_2017.iloc[:, :-12]\n",
    "\n",
    "X_train_2016 = model_train_2016.iloc[:, :-12]\n",
    "X_test_2016 = model_test_2016.iloc[:, :-12]\n",
    "\n",
    "features_num_2017 = []\n",
    "features_cat_2017 = []\n",
    "for feature in X_test_2017.columns:\n",
    "    if (X_test_2017[feature].dtype == 'float64') or (X_test_2017[feature].dtype == 'int64'):\n",
    "        features_num_2017.append(feature)\n",
    "    else:\n",
    "        features_cat_2017.append(feature)\n",
    "        \n",
    "features_num_2016 = []\n",
    "features_cat_2016 = []\n",
    "for feature in X_test_2016.columns:\n",
    "    if (X_test_2016[feature].dtype == 'float64') or (X_test_2016[feature].dtype == 'int64'):\n",
    "        features_num_2016.append(feature)\n",
    "    else:\n",
    "        features_cat_2016.append(feature)\n",
    "\n",
    "comp_train_2017 = X_train_2017[features_cat_2017[:4]]\n",
    "comp_test_2017 = X_test_2017[features_cat_2017[:4]]\n",
    "\n",
    "comp_train_2016 = X_train_2016[features_cat_2016[:4]]\n",
    "comp_test_2016 = X_test_2016[features_cat_2016[:4]]\n",
    "\n",
    "comp_train_2017['year'] = pd.DatetimeIndex(comp_train_2017['DataDate']).year\n",
    "comp_test_2017['year'] = pd.DatetimeIndex(comp_test_2017['DataDate']).year\n",
    "\n",
    "comp_train_2016['year'] = pd.DatetimeIndex(comp_train_2016['DataDate']).year\n",
    "comp_test_2016['year'] = pd.DatetimeIndex(comp_test_2016['DataDate']).year\n",
    "\n",
    "X_train_num_2017 = X_train_2017[features_num_2017]\n",
    "X_test_num_2017 = X_test_2017[features_num_2017]\n",
    "\n",
    "X_train_num_2016 = X_train_2016[features_num_2016]\n",
    "X_test_num_2016 = X_test_2016[features_num_2016]\n",
    "\n",
    "'''1. Кодируем категориальные признаки 2017'''\n",
    "\n",
    "X_sec_2017 = model[features_cat_2017[3:]]\n",
    "label_encoder_2017 = preprocessing.LabelEncoder() \n",
    "X_sec_enc_2017 = label_encoder_2017.fit_transform(X_sec_2017) \n",
    "\n",
    "X_sec_enc_train_2017 = X_sec_enc_2017[:X_train_num_2017.shape[0]]\n",
    "X_sec_enc_test_2017 = X_sec_enc_2017[X_train_num_2017.shape[0]:X_train_num_2017.shape[0]+X_test_num_2017.shape[0]]\n",
    "\n",
    "'''1. Кодируем категориальные признаки _2016'''\n",
    "\n",
    "X_sec_2016 = model[features_cat_2016[3:]]\n",
    "label_encoder_2016 = preprocessing.LabelEncoder() \n",
    "X_sec_enc_2016 = label_encoder_2016.fit_transform(X_sec_2016) \n",
    "\n",
    "X_sec_enc_train_2016 = X_sec_enc_2016[:X_train_num_2016.shape[0]]\n",
    "X_sec_enc_test_2016 = X_sec_enc_2016[X_train_num_2016.shape[0]:X_train_num_2016.shape[0]+X_test_num_2016.shape[0]]\n",
    "\n",
    "'''3. Масштабируем реальные признаки _2017'''\n",
    "\n",
    "scaler_2017 = StandardScaler()\n",
    "X_num_train_scaled_2017 = scaler_2017.fit_transform(X_train_num_2017)\n",
    "X_num_test_scaled_2017 = scaler_2017.transform(X_test_num_2017)\n",
    "\n",
    "'''3. Масштабируем реальные признакиь _2016'''\n",
    "\n",
    "scaler_2016 = StandardScaler()\n",
    "X_num_train_scaled_2016 = scaler_2016.fit_transform(X_train_num_2016)\n",
    "X_num_test_scaled_2016 = scaler_2016.transform(X_test_num_2016)\n",
    "\n",
    "\"\"\"4. Стакаем категориальные и реальные фичи _2017\"\"\"\n",
    "\n",
    "train_sample_2017 = np.hstack((X_num_train_scaled_2017, X_sec_enc_train_2017.reshape(-1, 1)))\n",
    "test_sample_2017 = np.hstack((X_num_test_scaled_2017, X_sec_enc_test_2017.reshape(-1, 1)))\n",
    "\n",
    "\"\"\"4. Стакаем категориальные и реальные фичи _2016\"\"\"\n",
    "\n",
    "train_sample_2016 = np.hstack((X_num_train_scaled_2016, X_sec_enc_train_2016.reshape(-1, 1)))\n",
    "test_sample_2016 = np.hstack((X_num_test_scaled_2016, X_sec_enc_test_2016.reshape(-1, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''1. Кодируем категориальные признаки lin'''\n",
    "X_sec_2017 = model[features_cat_2017[3:]]\n",
    "encoder_2017 = DV(sparse = False)\n",
    "X_sec_enc_2017_lin = encoder_2017.fit_transform(X_sec_2017.T.to_dict().values())\n",
    "\n",
    "X_sec_2016 = model[features_cat_2016[3:]]\n",
    "encoder_2016 = DV(sparse = False)\n",
    "X_sec_enc_2016_lin = encoder_2016.fit_transform(X_sec_2016.T.to_dict().values())\n",
    "\n",
    "X_sec_enc_train_2017_lin = X_sec_enc_2017_lin[:X_train_num_2017.shape[0]]\n",
    "X_sec_enc_test_2017_lin = X_sec_enc_2017_lin[X_train_num_2017.shape[0]:X_train_num_2017.shape[0]+X_test_num_2017.shape[0]]\n",
    "\n",
    "X_sec_enc_train_2016_lin = X_sec_enc_2016_lin[:X_train_num_2016.shape[0]]\n",
    "X_sec_enc_test_2016_lin = X_sec_enc_2016_lin[X_train_num_2016.shape[0]:X_train_num_2016.shape[0]+X_test_num_2016.shape[0]]\n",
    "\n",
    "\"\"\"4. Стакаем категориальные и реальные фичи lin\"\"\"\n",
    "X_num_train_scaled_2016 = X_num_train_scaled_2016.T[l[:-1]].T\n",
    "X_num_test_scaled_2016 = X_num_test_scaled_2016.T[l[:-1]].T\n",
    "\n",
    "train_sample_lin_2016 = np.hstack((X_num_train_scaled_2016, X_sec_enc_train_2016_lin))\n",
    "test_sample_lin_2016 = np.hstack((X_num_test_scaled_2016, X_sec_enc_test_2016_lin))\n",
    "\n",
    "\"\"\"4. Стакаем категориальные и реальные фичи lin\"\"\"\n",
    "\n",
    "X_num_train_scaled_2017 = X_num_train_scaled_2017.T[l[:-1]].T\n",
    "X_num_test_scaled_2017 = X_num_test_scaled_2017.T[l[:-1]].T\n",
    "\n",
    "train_sample_lin_2017 = np.hstack((X_num_train_scaled_2017, X_sec_enc_train_2017_lin))\n",
    "test_sample_lin_2017 = np.hstack((X_num_test_scaled_2017, X_sec_enc_test_2017_lin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample_2017 = train_sample_2017.T[l].T\n",
    "test_sample_2017 = test_sample_2017.T[l].T\n",
    "\n",
    "train_sample_2016 = train_sample_2016.T[l].T\n",
    "test_sample_2016 = test_sample_2016.T[l].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lin_6m_2017 = LogisticRegression(C=0.05, class_weight=None, dual=False, fit_intercept=True,\n",
    "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
    "                   multi_class='auto', n_jobs=None, penalty='l1',\n",
    "                   random_state=123, solver='liblinear', tol=0.0001, verbose=0,\n",
    "                   warm_start=False)\n",
    "\n",
    "\n",
    "best_lin_6m_2016 = LogisticRegression(C=0.05, class_weight=None, dual=False, fit_intercept=True,\n",
    "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
    "                   multi_class='auto', n_jobs=None, penalty='l1',\n",
    "                   random_state=123, solver='liblinear', tol=0.0001, verbose=0,\n",
    "                   warm_start=False)\n",
    "\n",
    "best_lin_6m_2017.fit(train_sample_lin_2017, Y_6_train_2017)\n",
    "best_lin_6m_2016.fit(train_sample_lin_2016, Y_6_train_2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Деревення модель _2017\n",
    "best_rfc_6m_2017 = RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
    "                       criterion='gini', max_depth=20, max_features=6,\n",
    "                       max_leaf_nodes=None, max_samples=None,\n",
    "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                       min_samples_leaf=13, min_samples_split=2,\n",
    "                       min_weight_fraction_leaf=0.0, n_estimators=35, n_jobs=-1,\n",
    "                       oob_score=True, random_state=123, verbose=0,\n",
    "                       warm_start=False)\n",
    "best_rfc_6m_2017.fit(train_sample_2017, Y_6_train_2017)\n",
    "\n",
    "# Деревення модель _2016\n",
    "best_rfc_6m_2016 = RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
    "                       criterion='gini', max_depth=20, max_features=6,\n",
    "                       max_leaf_nodes=None, max_samples=None,\n",
    "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                       min_samples_leaf=13, min_samples_split=2,\n",
    "                       min_weight_fraction_leaf=0.0, n_estimators=35, n_jobs=-1,\n",
    "                       oob_score=True, random_state=123, verbose=0,\n",
    "                       warm_start=False)\n",
    "best_rfc_6m_2016.fit(train_sample_2016, Y_6_train_2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# модель XGBoost _2017\n",
    "best_xgb_6m_opt_2017 = xgb.XGBClassifier(base_score=0.5, booster=None, colsample_bylevel=1,\n",
    "              colsample_bynode=1, colsample_bytree=0.7, gamma=0.5, gpu_id=-1,\n",
    "              importance_type='gain', interaction_constraints=None,\n",
    "              learning_rate=0.1, max_delta_step=0, max_depth=7,\n",
    "              min_child_weight=9, missing=nan, monotone_constraints=None,\n",
    "              n_estimators=100, n_jobs=0, num_parallel_tree=1,\n",
    "              objective='binary:logistic', random_state=123, reg_alpha=0,\n",
    "              reg_lambda=1, scale_pos_weight=1, silent=1, subsample=0.6,\n",
    "              tree_method=None, validate_parameters=False, verbosity=None)\n",
    "best_xgb_6m_opt_2017.fit(train_sample_2017, Y_6_train_2017)\n",
    "\n",
    "# модель XGBoost _2016\n",
    "best_xgb_6m_opt_2016 = xgb.XGBClassifier(base_score=0.5, booster=None, colsample_bylevel=1,\n",
    "              colsample_bynode=1, colsample_bytree=0.7, gamma=0.5, gpu_id=-1,\n",
    "              importance_type='gain', interaction_constraints=None,\n",
    "              learning_rate=0.1, max_delta_step=0, max_depth=7,\n",
    "              min_child_weight=9, missing=nan, monotone_constraints=None,\n",
    "              n_estimators=100, n_jobs=0, num_parallel_tree=1,\n",
    "              objective='binary:logistic', random_state=123, reg_alpha=0,\n",
    "              reg_lambda=1, scale_pos_weight=1, silent=1, subsample=0.6,\n",
    "              tree_method=None, validate_parameters=False, verbosity=None)\n",
    "best_xgb_6m_opt_2016.fit(train_sample_2016, Y_6_train_2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализируем страифицированную разбивку нашего датасета для валидации _2017\n",
    "skf = StratifiedKFold(n_splits = 5, random_state = 123)\n",
    "\n",
    "rfc_12_2017 = RandomForestClassifier(random_state = 123, n_jobs = -1, oob_score = True)\n",
    "\n",
    "results_12_2017 = cross_val_score(rfc_12_2017, train_sample_2017, Y_6_train_2017, cv=skf)\n",
    "\n",
    "# Оцениваем долю верных ответов на тестовом датасете\n",
    "print(\"CV accuracy score model_6m_2017: {:.2f}%\".format(results_12_2017.mean()*100))\n",
    "\n",
    "\n",
    "# Инициализируем страифицированную разбивку нашего датасета для валидации _2016\n",
    "skf = StratifiedKFold(n_splits = 5, random_state = 123)\n",
    "\n",
    "rfc_12_2016 = RandomForestClassifier(random_state = 123, n_jobs = -1, oob_score = True)\n",
    "\n",
    "results_12_2016 = cross_val_score(rfc_12_2016, train_sample_2016, Y_6_train_2016, cv=skf)\n",
    "\n",
    "# Оцениваем долю верных ответов на тестовом датасете\n",
    "print(\"CV accuracy score model_6m_2016: {:.2f}%\".format(results_12_2016.mean()*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LR 2017\n",
    "proba_l_2017 = best_lin_6m_2017.predict_proba(test_sample_lin_2017)\n",
    "auc_l_2017 = roc_auc_score(np.array(Y_6_test_2017), np.transpose(proba_l_2017)[1])\n",
    "print('auc_l_2017 {:.5}'.format(auc_l_2017), '\\n')\n",
    "fpr_l_2017, tpr_l_2017, _ = roc_curve(Y_6_test_2017, proba_l_2017[:,1])\n",
    "\n",
    "# LR 2016\n",
    "proba_l_2016 = best_lin_6m_2016.predict_proba(test_sample_lin_2016)\n",
    "auc_l_2016 = roc_auc_score(np.array(Y_6_test_2016), np.transpose(proba_l_2016)[1])\n",
    "print('auc_l_2016 {:.5}'.format(auc_l_2016), '\\n')\n",
    "fpr_l_2016, tpr_l_2016, _ = roc_curve(Y_6_test_2016, proba_l_2016[:,1])\n",
    "\n",
    "# Forest _2017\n",
    "proba_F_2017 = best_rfc_6m_2017.predict_proba(test_sample_2017)\n",
    "auc_F_2017 = roc_auc_score(np.array(Y_6_test_2017), np.transpose(proba_F_2017)[1])\n",
    "print('auc_F_2017 {:.5}'.format(auc_F_2017), '\\n')\n",
    "fpr_F_2017, tpr_F_2017, _ = roc_curve(Y_6_test_2017, proba_F_2017[:,1])\n",
    "\n",
    "# Forest _2016\n",
    "proba_F_2016 = best_rfc_6m_2016.predict_proba(test_sample_2016)\n",
    "auc_F_2016 = roc_auc_score(np.array(Y_6_test_2016), np.transpose(proba_F_2016)[1])\n",
    "print('auc_F_2016 {:.5}'.format(auc_F_2016), '\\n')\n",
    "fpr_F_2016, tpr_F_2016, _ = roc_curve(Y_6_test_2016, proba_F_2016[:,1])\n",
    "\n",
    "# Boost _2017\n",
    "proba_B_2017 = best_xgb_6m_opt_2017.predict_proba(test_sample_2017)\n",
    "auc_B_2017 = roc_auc_score(np.array(Y_6_test_2017), np.transpose(proba_B_2017)[1])\n",
    "print('auc_B_2017 {:.5}'.format(auc_B_2017), '\\n')\n",
    "fpr_B_2017, tpr_B_2017, _ = roc_curve(Y_6_test_2017, proba_B_2017[:,1])\n",
    "\n",
    "# Boost _2016\n",
    "proba_B_2016 = best_xgb_6m_opt_2016.predict_proba(test_sample_2016)\n",
    "auc_B_2016 = roc_auc_score(np.array(Y_6_test_2016), np.transpose(proba_B_2016)[1])\n",
    "print('auc_B_2016 {:.5}'.format(auc_B_2016), '\\n')\n",
    "fpr_B_2016, tpr_B_2016, _ = roc_curve(Y_6_test_2016, proba_B_2016[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _2017\n",
    "\n",
    "fig = plt.figure(figsize=(35, 10))\n",
    "sns.set(style=\"whitegrid\", color_codes=True, font_scale = 2)\n",
    "ax13 = fig.add_subplot(131);ax14 = fig.add_subplot(132); ax15 = fig.add_subplot(133);\n",
    "\n",
    "ax13.plot(fpr_F_2017, tpr_F_2017, label = 'Forest_2018')\n",
    "ax13.plot([0, 1], [0, 1], '--', color = 'grey', label = 'random')\n",
    "ax13.set_xlim([-0.05, 1.05])\n",
    "ax13.set_ylim([-0.05, 1.05])\n",
    "ax13.set_xlabel('False Positive Rate')\n",
    "ax13.set_ylabel('True Positive Rate')\n",
    "ax13.set_title('ROC curve_2018 (AUC_ROC = 0.62)')\n",
    "ax13.legend(loc = \"lower right\")\n",
    "\n",
    "ax14.plot(fpr_B_2017, tpr_B_2017, label = 'Boost_2018')\n",
    "ax14.plot([0, 1], [0, 1], '--', color = 'grey', label = 'random')\n",
    "ax14.set_xlim([-0.05, 1.05])\n",
    "ax14.set_ylim([-0.05, 1.05])\n",
    "ax14.set_xlabel('False Positive Rate')\n",
    "ax14.set_ylabel('True Positive Rate')\n",
    "ax14.set_title('ROC curve_2018 (AUC_ROC = 0.56)')\n",
    "ax14.legend(loc = \"lower right\")\n",
    "\n",
    "ax15.plot(fpr_l_2017, tpr_l_2017, label = 'LR_2018')\n",
    "ax15.plot([0, 1], [0, 1], '--', color = 'grey', label = 'random')\n",
    "ax15.set_xlim([-0.05, 1.05])\n",
    "ax15.set_ylim([-0.05, 1.05])\n",
    "ax15.set_xlabel('False Positive Rate')\n",
    "ax15.set_ylabel('True Positive Rate')\n",
    "ax15.set_title('ROC curve_2018 (AUC_ROC = 0.545)')\n",
    "ax15.legend(loc = \"lower right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _2016\n",
    "\n",
    "fig = plt.figure(figsize=(35, 10))\n",
    "ax13 = fig.add_subplot(131);ax14 = fig.add_subplot(132); ax15 = fig.add_subplot(133);\n",
    "sns.set(style=\"whitegrid\", color_codes=True, font_scale = 2)\n",
    "\n",
    "ax13.plot(fpr_F_2016, tpr_F_2016, label = 'Forest_2017')\n",
    "ax13.plot([0, 1], [0, 1], '--', color = 'grey', label = 'random')\n",
    "ax13.set_xlim([-0.05, 1.05])\n",
    "ax13.set_ylim([-0.05, 1.05])\n",
    "ax13.set_xlabel('False Positive Rate')\n",
    "ax13.set_ylabel('True Positive Rate')\n",
    "ax13.set_title('ROC curve_2017 (AUC_ROC = 0.545)')\n",
    "ax13.legend(loc = \"lower right\")\n",
    "\n",
    "ax14.plot(fpr_B_2016, tpr_B_2016, label = 'Boost_2017')\n",
    "ax14.plot([0, 1], [0, 1], '--', color = 'grey', label = 'random')\n",
    "ax14.set_xlim([-0.05, 1.05])\n",
    "ax14.set_ylim([-0.05, 1.05])\n",
    "ax14.set_xlabel('False Positive Rate')\n",
    "ax14.set_ylabel('True Positive Rate')\n",
    "ax14.set_title('ROC curve_2017 (AUC_ROC = 0.55)')\n",
    "ax14.legend(loc = \"lower right\")\n",
    "\n",
    "ax15.plot(fpr_l_2016, tpr_l_2016, label = 'LR_2017')\n",
    "ax15.plot([0, 1], [0, 1], '--', color = 'grey', label = 'random')\n",
    "ax15.set_xlim([-0.05, 1.05])\n",
    "ax15.set_ylim([-0.05, 1.05])\n",
    "ax15.set_xlabel('False Positive Rate')\n",
    "ax15.set_ylabel('True Positive Rate')\n",
    "ax15.set_title('ROC curve_2017 (AUC_ROC = 0.56)')\n",
    "ax15.legend(loc = \"lower right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean _2017\n",
    "proba_F_2017 = best_rfc_6m_2017.predict_proba(test_sample_2017)\n",
    "proba_B_2017 = best_xgb_6m_opt_2017.predict_proba(test_sample_2017)\n",
    "proba_l_2017 = best_lin_6m_2017.predict_proba(test_sample_lin_2017)\n",
    "proba_M_2017 = (proba_F_2017 + proba_B_2017 + proba_l_2017) / 3\n",
    "\n",
    "auc_M_2017 = roc_auc_score(np.array(Y_6_test_2017), np.transpose(proba_M_2017)[1])\n",
    "print('auc_M_2017 {:.5}'.format(auc_M_2017), '\\n')\n",
    "fpr_M_2017, tpr_M_2017, _ = roc_curve(Y_6_test_2017, proba_M_2017[:,1])\n",
    "\n",
    "# Mean _2016\n",
    "proba_F_2016 = best_rfc_6m_2016.predict_proba(test_sample_2016)\n",
    "proba_B_2016 = best_xgb_6m_opt_2016.predict_proba(test_sample_2016)\n",
    "proba_l_2016 = best_lin_6m_2016.predict_proba(test_sample_lin_2016)\n",
    "proba_M_2016 = (proba_F_2016 + proba_B_2016 + proba_l_2016) / 3\n",
    "\n",
    "auc_M_2016 = roc_auc_score(np.array(Y_6_test_2016), np.transpose(proba_M_2016)[1])\n",
    "print('auc_M_2016 {:.5}'.format(auc_M_2016), '\\n')\n",
    "fpr_M_2016, tpr_M_2016, _ = roc_curve(Y_6_test_2016, proba_M_2016[:,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "7# _2017\n",
    "\n",
    "fig = plt.figure(figsize=(32, 11))\n",
    "ax13 = fig.add_subplot(121);ax14 = fig.add_subplot(122);\n",
    "\n",
    "ax13.plot(fpr_M_2017, tpr_M_2017, label = 'stack_2018')\n",
    "ax13.plot([0, 1], [0, 1], '--', color = 'grey', label = 'random')\n",
    "ax13.set_xlim([-0.05, 1.05])\n",
    "ax13.set_ylim([-0.05, 1.05])\n",
    "ax13.set_xlabel('False Positive Rate')\n",
    "ax13.set_ylabel('True Positive Rate')\n",
    "ax13.set_title('ROC curve_2018 (AUC_ROC = 0.59)')\n",
    "ax13.legend(loc = \"lower right\")\n",
    "\n",
    "# _2016\n",
    "\n",
    "ax14.plot(fpr_M_2016, tpr_M_2016, label = 'stack_2017')\n",
    "ax14.plot([0, 1], [0, 1], '--', color = 'grey', label = 'random')\n",
    "ax14.set_xlim([-0.05, 1.05])\n",
    "ax14.set_ylim([-0.05, 1.05])\n",
    "ax14.set_xlabel('False Positive Rate')\n",
    "ax14.set_ylabel('True Positive Rate')\n",
    "ax14.set_title('ROC curve_2017 (AUC_ROC = 0.56)')\n",
    "ax14.legend(loc = \"lower right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_2016 = pd.DataFrame(proba_M_2016, columns = ['0','1'])\n",
    "comp_test_2016['Prob_1'] =  np.array(probs_2016.iloc[:, 1])\n",
    "\n",
    "true_2016 = Y_test_2016.loc[:, ['shifted_6', 'shifted_6_sec', 'y_6']]\n",
    "\n",
    "comp_test_2016['shifted_6'] = np.array(true_2016.iloc[:, 0])\n",
    "comp_test_2016['shifted_6_sec'] = np.array(true_2016.iloc[:, 1])\n",
    "comp_test_2016['y_6'] = np.array(true_2016.iloc[:, 2])\n",
    "comp_test_2016.reset_index(inplace = True, drop = True)\n",
    "#comp_test_hard_2016 = comp_test_2016[comp_test_2016.Prob_1 >= .6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_2017 = pd.DataFrame(proba_M_2017, columns = ['0','1'])\n",
    "comp_test_2017['Prob_1'] =  np.array(probs_2017.iloc[:, 1])\n",
    "\n",
    "true_2017 = Y_test_2017.loc[:, ['shifted_6', 'shifted_6_sec', 'y_6']]\n",
    "\n",
    "comp_test_2017['shifted_6'] = np.array(true_2017.iloc[:, 0])\n",
    "comp_test_2017['shifted_6_sec'] = np.array(true_2017.iloc[:, 1])\n",
    "comp_test_2017['y_6'] = np.array(true_2017.iloc[:, 2])\n",
    "comp_test_2017.reset_index(inplace = True, drop = True)\n",
    "# comp_test_hard_2017 = comp_test_2017[comp_test_2017.Prob_1 >= .55]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_test_2016['rating'] = 0\n",
    "i = 1\n",
    "alpha = 0\n",
    "while alpha <= 0.9:     \n",
    "    boundaries = np.percentile(comp_test_2016.Prob_1, [100 * alpha, 100 * (alpha + 0.1)])\n",
    "    comp_test_2016['rating'][(comp_test_2016.Prob_1 >= boundaries[0]) & (comp_test_2016.Prob_1 <= boundaries[1])] = i\n",
    "    alpha += 0.1\n",
    "    i += 1\n",
    "\n",
    "comp_test_2017['rating'] = 0\n",
    "i = 1\n",
    "alpha = 0\n",
    "while alpha <= 0.9:     \n",
    "    boundaries = np.percentile(comp_test_2017.Prob_1, [100 * alpha, 100 * (alpha + 0.1)])\n",
    "    comp_test_2017['rating'][(comp_test_2017.Prob_1 >= boundaries[0]) & (comp_test_2017.Prob_1 <= boundaries[1])] = i\n",
    "    alpha += 0.1\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_test_2017.sort_values(by = ['rating', 'Prob_1'], ascending = False, inplace = True)\n",
    "comp_test_2016.sort_values(by = ['rating', 'Prob_1'], ascending = False, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_2017 = comp_test_2017[comp_test_2017.rating == 10]\n",
    "top_10_2017.to_excel('top_10_2017.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series.value_counts(top_10_2017['Sector'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_2016 = comp_test_2016[comp_test_2016.rating == 10]\n",
    "top_10_2016.to_excel('top_10_2016.xlsx') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series.value_counts(top_10_2016['Sector'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_sec_mean_2016 = pd.DataFrame()\n",
    "prob_sec_mean_2016['num'] = ceil((pd.Series.value_counts(comp_test_hard_2016['Sector']) / comp_test_hard_2016['Sector'].count()) * 20)\n",
    "prob_sec_mean_2016.reset_index(inplace = True)\n",
    "prob_sec_mean_2016.rename(columns = {'index': 'Sector'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series.value_counts(itog_2016['Sector'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_sec_mean_2017 = pd.DataFrame()\n",
    "prob_sec_mean_2017['num'] = ceil((pd.Series.value_counts(comp_test_hard_2017['Sector']) / comp_test_hard_2017['Sector'].count()) * 20)\n",
    "prob_sec_mean_2017.reset_index(inplace = True)\n",
    "prob_sec_mean_2017.rename(columns = {'index': 'Sector'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_2016 = pd.DataFrame()\n",
    "for sector in list(set(prob_sec_mean_2016.Sector)):\n",
    "    l_2016 = comp_test_hard_2016[comp_test_hard_2016.Sector == sector]\n",
    "    l_2016 = l_2016.sort_values(by = [ 'Prob_1'], ascending = False).head(int(prob_sec_mean_2016['num'][prob_sec_mean_2016.Sector == sector]))\n",
    "    new_2016 = pd.concat([new_2016, l_2016])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_2017 = pd.DataFrame()\n",
    "for sector in list(set(prob_sec_mean_2017.Sector)):\n",
    "    l_2017 = comp_test_hard_2017[comp_test_hard_2017.Sector == sector]\n",
    "    l_2017 = l_2017.sort_values(by = [ 'Prob_1'], ascending = False).head(int(prob_sec_mean_2017['num'][prob_sec_mean_2017.Sector == sector]))\n",
    "    new_2017 = pd.concat([new_2017, l_2017])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itog_2016 = new_2016.sort_values(by = [ 'Prob_1','Sector'], ascending = False)\n",
    "itog_2017 = new_2017.sort_values(by = [ 'Prob_1','Sector'], ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itog_2016.to_excel('proba_2016.xlsx')\n",
    "itog_2017.to_excel('proba_2017.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
